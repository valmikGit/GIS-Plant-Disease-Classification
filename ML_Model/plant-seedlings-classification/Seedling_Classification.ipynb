{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to classify images of seedlings into 1 of 12 possible species. After some initial data exploration, I will perform some pre-processing using computer vision techniques in order to accelerate the training process. Finally, I will train a CNN model and generate a submission file containing predictions for the test set provided by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import random\n",
    "import pickle\n",
    "from keras import models, layers, callbacks\n",
    "import shutil\n",
    "import cv2\n",
    "from math import sqrt, floor\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets have a look at the directory structure seen below. The three dots (...) indicate that there were too many files in the directory to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bold(text):\n",
    "    print('\\033[1m{}\\033[0m'.format(text))\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        \n",
    "        dir_name= '{}{}/'.format(indent, os.path.basename(root))\n",
    "        if dir_name.strip().startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        print_bold('\\n'+dir_name)\n",
    "            \n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        if level==0:\n",
    "            for f in files:\n",
    "                if f.startswith('.'):\n",
    "                    continue\n",
    "                print('{}{}'.format(subindent, f))\n",
    "        else:\n",
    "            #if len(files)>0:\n",
    "            #   print('{}File Count: {}'.format(subindent, len(files)))\n",
    "            for i, f in enumerate(files):\n",
    "                print('{}{}'.format(subindent, f))\n",
    "                if i==2:\n",
    "                    print('{}{}'.format(subindent, '...'))\n",
    "                    break\n",
    "\n",
    "list_files(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parent folder of our training images indicates the class label. Lets have a look at the count of records for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes= []\n",
    "sample_counts= []\n",
    "\n",
    "for f in os.listdir('train'):\n",
    "    train_class_path= os.path.join('train', f)\n",
    "    if os.path.isdir(train_class_path):\n",
    "        classes.append(f)\n",
    "        sample_counts.append(len(os.listdir(train_class_path)))\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Example data\n",
    "y_pos = np.arange(len(classes))\n",
    "\n",
    "ax.barh(y_pos, sample_counts, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(classes)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Sample Counts')\n",
    "ax.set_title('Sample Counts Per Class')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, there is some class inbalance. I will account for this by defining class weights at the time of training. Lets have a look at a few random images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize= (10, 15))\n",
    "fig.suptitle('Random Samples From Each Class', fontsize=14, y=.92, horizontalalignment='center', weight='bold')\n",
    "\n",
    "columns = 5\n",
    "rows = 12\n",
    "for i in range(12):\n",
    "    sample_class= os.path.join('train',classes[i])\n",
    "    for j in range(1,6):\n",
    "        fig.add_subplot(rows, columns, i*5+j)\n",
    "        plt.axis('off')\n",
    "        if j==1:\n",
    "            plt.text(0.0, 0.5,str(classes[i]).replace(' ','\\n'), fontsize=13, wrap=True)\n",
    "            continue\n",
    "        random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n",
    "        #from keras.preprocessing.image\n",
    "        img = image.load_img(random_image, target_size=(150, 150))\n",
    "        img= image.img_to_array(img)\n",
    "        img/=255.\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some important things to note here. We see that the resolution of these images can vary significantly. We also see that images for a particular specied seem to have been taken at various points of its life cycle. Luckily, all of the images share a roughly similar background which may allow for us to remove it from the image in order to accelerate training.\n",
    "\n",
    "Before doing this, we must address the fact that there is no validation dataset yet. I will construct a validation set using 20% of the training set. In order to maintain the same distribution, I will randomly select 20% from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation set\n",
    "def create_validation(validation_split=0.2):\n",
    "    if os.path.isdir('validation'):\n",
    "        print('Validation directory already created!')\n",
    "        print('Process Terminated')\n",
    "        return\n",
    "    os.mkdir('validation')\n",
    "    for f in os.listdir('train'):\n",
    "        train_class_path= os.path.join('train', f)\n",
    "        if os.path.isdir(train_class_path):\n",
    "            validation_class_path= os.path.join('validation', f)\n",
    "            os.mkdir(validation_class_path)\n",
    "            files_to_move= int(0.2*len(os.listdir(train_class_path)))\n",
    "            \n",
    "            for i in range(files_to_move):\n",
    "                random_image= os.path.join(train_class_path, random.choice(os.listdir(train_class_path)))\n",
    "                shutil.move(random_image, validation_class_path)\n",
    "    print('Validation set created successfully using {:.2%} of training data'.format(validation_split))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look at a bar chart showing counts for each class in our training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts= {}\n",
    "\n",
    "for i, d in enumerate(['train', 'validation']):\n",
    "\n",
    "    classes= []\n",
    "    sample_counts[d]= []\n",
    "\n",
    "    for f in os.listdir(d):\n",
    "        train_class_path= os.path.join(d, f)\n",
    "        if os.path.isdir(train_class_path):\n",
    "            classes.append(f)\n",
    "            sample_counts[d].append(len(os.listdir(train_class_path)))\n",
    "\n",
    "    #fig, ax= plt.subplot(221+i)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Example data\n",
    "    y_pos = np.arange(len(classes))\n",
    "\n",
    "    ax.barh(y_pos, sample_counts[d], align='center')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(classes)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Sample Counts')\n",
    "    ax.set_title('{} Sample Counts Per Class'.format(d.capitalize()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class with the fewest number of records in our validation set has just over 40 records. Although this is less than ideal, it should still be sufficient to get a decent measure of our model's accuracy.\n",
    "\n",
    "Now I will attempt to remove the background from the images to see if can find a method which generalizes well across all images, then this can be used to accelerate training by isolating the important part of our data. The strategy will be to find upper and lower bounds within a color space which will only contain the green part of the plants. We will then turn the rest of the background black. In order to find the best values for these upper and lower bounds, I grab random pixels from random training images from each of my 12 classes. I will then take this random collection of pixels and plot it in color space i hopes that I can find upper and lower bounds which cleanly seperate the green part of the plants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_random_pixels(samples_per_class, pixels_per_sample):\n",
    "    total_pixels= 12*samples_per_class*pixels_per_sample\n",
    "    random_pixels= np.zeros((total_pixels, 3), dtype=np.uint8)\n",
    "    for i in range(12):\n",
    "        sample_class= os.path.join('train',classes[i])\n",
    "        for j in range(samples_per_class):\n",
    "            \n",
    "            random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n",
    "            img= cv2.imread(random_image)\n",
    "            img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img=np.reshape(img, (img.shape[0]*img.shape[1], 3))\n",
    "            new_pixels= img[np.random.randint(0, img.shape[0], pixels_per_sample)]\n",
    "            \n",
    "            start_index=pixels_per_sample*(i*samples_per_class+j)\n",
    "            random_pixels[start_index:start_index+pixels_per_sample,:]= new_pixels\n",
    "\n",
    "    h= floor(sqrt(total_pixels))\n",
    "    w= total_pixels//h\n",
    "    \n",
    "    random_pixels= random_pixels[np.random.choice(total_pixels, h*w, replace=False)]\n",
    "    random_pixels= np.reshape(random_pixels, (h, w, 3))\n",
    "    return random_pixels\n",
    "    \n",
    "random_pixels= pull_random_pixels(10, 50)\n",
    "\n",
    "plt.figure()\n",
    "plt.suptitle('Random Samples From Each Class', fontsize=14, horizontalalignment='center')\n",
    "plt.imshow(random_pixels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above is a random sampling of 50 pixels from each of 10 training images from each of 12 classes. I will now plot these pixels in color space (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "\n",
    "r, g, b = cv2.split(random_pixels)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "axis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "axis.view_init(20, 120)\n",
    "\n",
    "pixel_colors = random_pixels.reshape((np.shape(random_pixels)[0]*np.shape(random_pixels)[1], 3))\n",
    "norm = colors.Normalize(vmin=-1.,vmax=1.)\n",
    "norm.autoscale(pixel_colors)\n",
    "pixel_colors = norm(pixel_colors).tolist()\n",
    "\n",
    "\n",
    "axis.scatter(r.flatten(), g.flatten(), b.flatten(), facecolors=pixel_colors, marker=\".\")\n",
    "axis.set_xlabel(\"Red\")\n",
    "axis.set_ylabel(\"Green\")\n",
    "axis.set_zlabel(\"Blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an initial view, it looks like the green regions may be seperable from the rest, but simply coosing bounds of RGB values will not work due to the shape of the distribution. Before resorting to more sophisticated methods to isolate these pixels, lets try a differe color space basis (HSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_img = cv2.cvtColor(np.uint8(random_pixels), cv2.COLOR_RGB2HSV)\n",
    "\n",
    "h, s, v = cv2.split(hsv_img)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "axis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "axis.view_init(50, 240)\n",
    "\n",
    "\n",
    "\n",
    "axis.scatter(h.flatten(), s.flatten(), v.flatten(), facecolors=pixel_colors, marker=\".\")\n",
    "axis.set_xlabel(\"Hue\")\n",
    "axis.set_ylabel(\"Saturation\")\n",
    "axis.set_zlabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HSV space, it looks like our clusters are more neatly seperable by choosing upper and lower bounds of HSV values. We can also clearly see that the green cluster comes from the plants and the brown, white, and grey clusters must come from the dirt, rocks, and other things in the background. It looks like there is little variance along the V (value) axis, so lets plot this in 2 dimensions (H & S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_img = cv2.cvtColor(np.uint8(random_pixels), cv2.COLOR_RGB2HSV)\n",
    "\n",
    "h, s, v = cv2.split(hsv_img)\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "axis = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "axis.scatter(h.flatten(), s.flatten(), facecolors=pixel_colors, marker=\".\")\n",
    "axis.set_xlabel(\"Hue\")\n",
    "axis.set_ylabel(\"Saturation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can point to an upper and lower bound. I will isolate pixels with Hue values ranging from 24 to 55 and Saturation values ranging from 50 to 255.\n",
    "\n",
    "I will grab a random image from each class and use the cv2 library to map the pixels in HSV space and black out the background using the bounds I've identified previously. I will display the original and transformed images next to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound= (24, 50, 0)\n",
    "upper_bound= (55, 255, 255)\n",
    "\n",
    "fig= plt.figure(figsize=(10, 10))\n",
    "fig.suptitle('Random Pre-Processed Image From Each Class', fontsize=14, y=.92, horizontalalignment='center', weight='bold')\n",
    "\n",
    "for i in range(12):\n",
    "    sample_class=os.path.join('train',classes[i])\n",
    "    random_image= os.path.join(sample_class, random.choice(os.listdir(sample_class)))\n",
    "    img= cv2.imread(random_image)\n",
    "    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img= cv2.resize(img, (150, 150))\n",
    "    \n",
    "    hsv_img= cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    mask = cv2.inRange(hsv_img, lower_bound, upper_bound)\n",
    "    result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "    fig.add_subplot(6, 4, i*2+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')    \n",
    "\n",
    "    fig.add_subplot(6, 4, i*2+2)\n",
    "    plt.imshow(result)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach seems to have worked well. I will use this in my model. Its important to note that this should <em>not</em> be done if we expect the background to vary in real data. For example, if we were developing an ML application for these specific gardeners, then this pre-processing should be fine since we expect real life data to have a background consistent with what we see here. If we were to generalize this application to seedling images with different backgrounds, using different cameras, etc. then our pre-processing technique may cause issues.\n",
    "\n",
    "I will create a function to make the above transformation compatible with the ImageDataGenerator object from Keras, which I will be using in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_segment_function(img_array):\n",
    "    img_array= np.rint(img_array)\n",
    "    img_array= img_array.astype('uint8')\n",
    "    hsv_img= cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n",
    "    mask = cv2.inRange(hsv_img, (24, 50, 0), (55, 255, 255))\n",
    "    result = cv2.bitwise_and(img_array, img_array, mask=mask)\n",
    "    result= result.astype('float64')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will take advantage of data augmentation during training. Our training generator will apply random flips and rotations to our images in addition to the background removal function I've defined above. I will also define a validation and testing generator. The advantage of using generators is that there we can do our training and validation in batches while also avoiding the need to load all of our data into memory at once. This is especially important when working with large datasets which may be too large to even store locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Use tensorflow.keras\n",
    "\n",
    "# Create the image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.0,\n",
    "      height_shift_range=0.0,\n",
    "      shear_range=0.0,\n",
    "      zoom_range=0.0,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      preprocessing_function=color_segment_function,\n",
    "      fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=color_segment_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "  'train',\n",
    "  target_size=(150, 150),\n",
    "  batch_size=20,\n",
    "  class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'test',\n",
    "        target_size=(150,150),\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now construct a set of class weights to be provided during training in order to account for the class imbalance I've noted earlier. The result will be a value will be centered around 1, which represents the weight that will be applied to the loss value for the corresponding class. A higher weight value will  essentially tell the optimizer to treat these class as more important when adjusting trainable parameters. \n",
    "\n",
    "Using a normalized inverse frequency will produce higher weights for under-represented classes, which will avoid situations in which our CNN gives preference to certain classes just because there are more records from that class. This set of class weights are provided to the model's fit_generator method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get class indices and labels. calculate class weight\n",
    "label_map = {}\n",
    "for k, v in train_generator.class_indices.items():\n",
    "    label_map[v]=k\n",
    "\n",
    "class_counts= pd.Series(train_generator.classes).value_counts()\n",
    "class_weight= {}\n",
    "\n",
    "for i, c in class_counts.items():\n",
    "    class_weight[i]= 1.0/c\n",
    "    \n",
    "norm_factor= np.mean(list(class_weight.values()))\n",
    "\n",
    "for k in class_counts.keys():\n",
    "    class_weight[k]= class_weight[k]/norm_factor\n",
    "\n",
    "t = PrettyTable(['class_index', 'class_label', 'class_weight'])\n",
    "for i in sorted(class_weight.keys()):\n",
    "    t.add_row([i, label_map[i], '{:.2f}'.format(class_weight[i])])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define my model below. I use 4 convolutional layers followed by a densely connected layer. I include dropout for each layer in order to avoid overfitting. Relu activation functions are used for all layers except the last one which requires a softmax activation in order to produce a result appropriate for multiclass classification.\n",
    "\n",
    "A fair amount of experimentation has been done, and the architecture below has yeilded the best results. Although I have not saved the configuration and results of previous models in any systematic way, these experiments have involved modifying things such as:\n",
    "\n",
    "-Number of layers<br />\n",
    "-Use of batch normalization<br />\n",
    "-Learning rate schedules<br />\n",
    "-Different optimizers (rmsprop, SGD, adadelta, etc.)<br />\n",
    "-Use of dropout<br />\n",
    "-Nodes per layer<br />\n",
    "-Activation function (leaky relu)<br />\n",
    "-Training image size (refit to 300X300 instead of 150X150<br />\n",
    "-Inclusion/exclusion of background removal<br />\n",
    "-Using a pre-trained model<br />\n",
    "\n",
    "A more systematic approach to will be a goal for future projects, perhaps employing a random search across hyperparameters. This will become more practical with access to GPUs and multiple nodes for parallel training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "\n",
    "model.add(layers.Dense(12, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the Adam optimizer along with ModelCheckpoint callback to save the best model during training. The model checkpoint considers the model with the lowest loss value to be the best. Since the Adam optimizer will dynamically adjust its learning rate throughout training, it is not advised to define a learning rate scheduler as I've done in earlier trials with SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks, optimizers  # Ensure to use tensorflow.keras\n",
    "\n",
    "# Updated ModelCheckpoint with the required .keras extension for the file path\n",
    "best_cb = callbacks.ModelCheckpoint(\n",
    "    'model_best.keras',  # Updated file extension\n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'  # Save at the end of each epoch\n",
    ")\n",
    "\n",
    "# Updated Adam optimizer syntax\n",
    "opt = optimizers.Adam(learning_rate=0.0005, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I train the model below over 50 epochs. Note that the steps per epoch must be defined explicitely when using generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model using fit() without the use_multiprocessing argument\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    class_weight=class_weight,\n",
    "    steps_per_epoch=190,\n",
    "    epochs=2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=48,\n",
    "    verbose=1,\n",
    "    callbacks=[best_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model which ws saved by our checkpoint callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model from training\n",
    "model= models.load_model('model_best.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also have the history of the model. I plot the loss and history of the model below over the 50 training epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save history\n",
    "with open('model_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['acc']\n",
    "# val_acc = history.history['val_acc']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Retrieve accuracy and loss history from the history object\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Determine the minimum length of training and validation data\n",
    "epochs = range(1, min(len(acc), len(val_acc)) + 1)\n",
    "acc = acc[:len(epochs)]\n",
    "val_acc = val_acc[:len(epochs)]\n",
    "loss = loss[:len(epochs)]\n",
    "val_loss = val_loss[:len(epochs)]\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
